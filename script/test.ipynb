{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27 18:59:56.439722 0 240 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/Desktop/Projects/Reinforcement learning/atari/script/replay_memory.py:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([self.get_memory(idx) for idx in sample_indices]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27 19:00:16.131597 1 502 11.0\n",
      "2020-06-27 19:00:35.800650 2 764 11.0\n",
      "2020-06-27 19:00:55.888822 3 1026 11.0\n",
      "2020-06-27 19:01:14.526393 4 1266 7.0\n",
      "2020-06-27 19:01:34.929615 5 1528 11.0\n",
      "2020-06-27 19:01:55.762331 6 1790 11.0\n",
      "2020-06-27 19:02:14.857065 7 2030 7.0\n",
      "2020-06-27 19:02:33.337022 8 2270 7.0\n",
      "2020-06-27 19:02:54.183588 9 2532 11.0\n",
      "2020-06-27 19:03:15.094732 10 2794 11.0\n",
      "2020-06-27 19:03:34.131156 11 3034 7.0\n",
      "2020-06-27 19:03:55.040188 12 3296 11.0\n",
      "2020-06-27 19:04:13.965244 13 3536 7.0\n",
      "2020-06-27 19:04:33.097300 14 3776 7.0\n",
      "0:04:40.902938\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from utils import preprocess_frame_v1, preprocess_frame_v2, preprocess_frame_v3, \\\n",
    "    choose_action, clip_reward, get_lin_anneal_eps\n",
    "from atari_model import atari_model, fit_batch\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)  \n",
    "tf.config.threading.set_inter_op_parallelism_threads(16)\n",
    "\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"                                  # Seems to reduce time\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"0\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"24\" #  MKL-DNN equivalent of intra_op_parallelism_threads\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "total_train_len = 15    # Total no. of episodes to train over\n",
    "max_episode_len = None     # Max no. of frames agent is allowed to see per episode\n",
    "state_len = 4              # No. of stacked frames that comprise a state\n",
    "train_interval = 1         # Every four actions a gradient descend step is performed\n",
    "burnin_sz = 250          # Replay mem. burn-in: random policy is run for this many frames, training starts after\n",
    "replay_mem_sz = 1000000    # Max no. of frames cached in replay memory\n",
    "batch_sz = 32              # No. of training cases (sample from replay mem.) for each SGD update\n",
    "disc_rate = 0.99           # Q-learning discount factor (gamma)\n",
    "seed = 1234\n",
    "\n",
    "\n",
    "# Initialize Atari environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "# Set seeds\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "# Initialize new Q-net model and replay memory\n",
    "model = atari_model(env.action_space.n)\n",
    "replay_mem = ReplayMemory(replay_mem_sz, state_len)\n",
    "frame_num = 0\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Start the fun\n",
    "for episode_num in range(total_train_len):\n",
    "    # Start a new game (episode)\n",
    "    init_frame = env.reset()\n",
    "    new_life = True\n",
    "    game_over = False\n",
    "    \n",
    "    # Keep track of episode's total reward\n",
    "    cum_reward = 0\n",
    "    \n",
    "    # Play episode until game over\n",
    "    while not game_over:\n",
    "        if new_life:\n",
    "            # Start a new life in the game\n",
    "            frame, _, game_over, info = env.step(1)  # Fire to start playing\n",
    "            lives = info['ale.lives']\n",
    "            frame = preprocess_frame_v1(frame)\n",
    "            state = np.stack(state_len * [frame], axis=2)\n",
    "        else:\n",
    "            state = np.append(state[:, :, 1:], frame[:, :, None], axis=2)\n",
    "        \n",
    "        # Get action\n",
    "        burnin_done = frame_num > burnin_sz\n",
    "        action = choose_action(env, model, state, 0)\n",
    "\n",
    "        # Take action\n",
    "        frame, reward, game_over, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # Process env outputs\n",
    "        frame = preprocess_frame_v1(frame)\n",
    "        reward = clip_reward(reward)\n",
    "        new_life = info['ale.lives'] < lives \n",
    "        lives = info['ale.lives']\n",
    "        \n",
    "        # Add new transition to replay memory\n",
    "        transition = (action, reward, game_over, new_life, frame)\n",
    "        replay_mem.store_memory(transition)\n",
    "                                \n",
    "        # After burn-in period, train every `train_interval`\n",
    "        if burnin_done and frame_num % train_interval == 0:\n",
    "            mini_batch = replay_mem.get_sample(batch_sz)\n",
    "            fit_batch(model, disc_rate, *mini_batch)\n",
    "         \n",
    "        # Increase frame it\n",
    "        frame_num += 1\n",
    "    \n",
    "    if episode_num % 1 == 0:\n",
    "         print(datetime.now(), episode_num, frame_num, cum_reward)\n",
    "\n",
    "\n",
    "elapsed_time = datetime.now() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
