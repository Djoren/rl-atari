{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import clone_model\n",
    "from tensorflow.keras.losses import Huber, CategoricalCrossentropy, KLDivergence\n",
    "\n",
    "import sys\n",
    "sys.path.append('../script')\n",
    "from utils import ( \n",
    "    preprocess_frame_v4, choose_action, get_lin_anneal_eps, sample_ran_action, \n",
    "    play_episode, EpisodeLogger, frame_max_pooling, FRAME_CROP_SETTINGS, \n",
    "    run_saliency_map, animate_episode, animate_episode_sal\n",
    ")\n",
    "from atari_model import (\n",
    "    atari_model, atari_model_dueling, atari_model_dueling, atari_model_distr,\n",
    "    fit_batch_DQNn_PER, fit_batch_DDQNn_PER, fit_batch_DDQNn_PER_DS, train_on_batch\n",
    ")\n",
    "from replay_memory import PrioritizedReplayMemory\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "total_train_len = 50_000      # Total no. of episodes to train\n",
    "max_episode_len = None       # Max no. of frames agent is allowed to see per episode [CURRENTLY UNUSED]\n",
    "state_len = 4                # No. of stacked frames that comprise a state\n",
    "train_interval = 4           # Every x actions a gradient descend step is performed\n",
    "tgt_update_interval = 10_000  # Interval in terms of no. of frames after we update target model weights\n",
    "eps_init = 1                 # Initial eps in eps-greedy exploration \n",
    "eps_final = 0.1              # Final eps in eps-greedy exploration\n",
    "eps_final_frame = 1_000_000    # No. of frame over which eps is linearly annealed to final eps\n",
    "replay_init_sz = 50_000        # Replay mem. initialization size: random policy is run for this many frames, training starts after\n",
    "replay_mem_sz = 1_000_000      # Max no. of frames cached in replay memory\n",
    "\n",
    "batch_sz = 32                # No. of training cases (sample from replay mem.) for each SGD update\n",
    "disc_rate = 0.99             # Q-learning discount factor (gamma)\n",
    "n_step = 3                   # Determines multi-step learning (n=1 is simply single step learning)\n",
    "# lr = 0.0000625               # Learning rate of CNN\n",
    "lr = 0.00025\n",
    "\n",
    "per_alpha = 0.5              # Exponent of priority probabilities\n",
    "# per_beta_rng = [0.4, 1]      # TODO: amend code to handle this\n",
    "per_beta = 0              # Exponent of importance sampling weights\n",
    "init_tds = False           # Whether to compute td-errors for initial replay memories\n",
    "crop_frame = True\n",
    "\n",
    "# Model variants (Rainbow)\n",
    "large_net = False\n",
    "double_learn = True\n",
    "dueling_net = False\n",
    "noisy_net = False\n",
    "distr_net = True\n",
    "\n",
    "if noisy_net:\n",
    "    eps_init = eps_final = 0\n",
    "    \n",
    "if distr_net:\n",
    "    N = 51  # No. of atoms for our discretized distr.\n",
    "    V_min, V_max = -10, 10  # Min and max of distribution support\n",
    "    Z = np.linspace(V_min, V_max, N)  # Value distribution (i.e. the atoms)\n",
    "    dZ = (V_max - V_min) / (N - 1)\n",
    "    Z_repN = np.repeat([Z], N, axis=0)  # Utility matrix to avoid recomputing later on\n",
    "    tgt_zeroing = False  # Toggles if we set y_tgt to 0 or y_pred for actions that were not taken\n",
    "    # loss = CategoricalCrossentropy()\n",
    "    loss = KLDivergence()\n",
    "else:\n",
    "    Z = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1234]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Atari environment\n",
    "env = gym.make('SpaceInvadersDeterministic-v4')\n",
    "\n",
    "# Set seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging and storage\n",
    "game = 'space_invaders'\n",
    "# model_name = 'DQN_cp_4f_50ri_3n_pr_cf_tf7_cc_dd_pp3_ds'\n",
    "model_name = 'test1_5'\n",
    "model_dir = f'../{game}/model/{model_name}'\n",
    "\n",
    "os.mkdir(model_dir)\n",
    "os.mkdir(model_dir + '/record')\n",
    "os.mkdir(model_dir + '/model')\n",
    "\n",
    "ep_log = EpisodeLogger(model_dir + '/episode_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = range(6)  # Use default action space https://www.gymlibrary.dev/environments/atari/space_invaders/\n",
    "M = len(action_space)\n",
    "kernel_init = 'he_normal'\n",
    "\n",
    "# Set frame crop configuration\n",
    "env.reset()\n",
    "frame = env.step(1)[0]\n",
    "crop_lims = FRAME_CROP_SETTINGS[game] if crop_frame else None\n",
    "frame_shape = preprocess_frame_v4(frame, crop_lims).shape\n",
    "state_shape = (*frame_shape, state_len)\n",
    "\n",
    "# Initialize online and behavorial network\n",
    "if dueling_net:\n",
    "    model = atari_model_dueling(M, lr, state_shape, kernel_init, noisy_net, large_net)\n",
    "elif distr_net:\n",
    "    model = atari_model_distr(N, M, loss, lr, state_shape, kernel_init, noisy_net)\n",
    "else:\n",
    "    model = atari_model(M, lr, state_shape, kernel_init, noisy_net, large_net)\n",
    "\n",
    "model_tgt = clone_model(model)  # Target network\n",
    "model_tgt.set_weights(model.get_weights())\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_mem = PrioritizedReplayMemory(replay_mem_sz, state_len, n_step, per_alpha, per_beta)\n",
    "frame_num = 0\n",
    "max_episode_reward = 0\n",
    "episode_start = 0\n",
    "init_done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_frames (InputLayer)      [(None, 95, 65, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 95, 65, 4)    0           ['input_frames[0][0]']           \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 22, 15, 16)   4112        ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 10, 6, 32)    8224        ['conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1920)         0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " hid (Dense)                    (None, 256)          491776      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " p_0 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " p_1 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " p_2 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " p_3 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " p_4 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " p_5 (Dense)                    (None, 51)           13107       ['hid[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 51)        0           ['p_0[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 51)        0           ['p_1[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 1, 51)        0           ['p_2[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 1, 51)        0           ['p_3[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 1, 51)        0           ['p_4[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 51)        0           ['p_5[0][0]']                    \n",
      "                                                                                                  \n",
      " p_concat (Concatenate)         (None, 6, 51)        0           ['reshape[0][0]',                \n",
      "                                                                  'reshape_1[0][0]',              \n",
      "                                                                  'reshape_2[0][0]',              \n",
      "                                                                  'reshape_3[0][0]',              \n",
      "                                                                  'reshape_4[0][0]',              \n",
      "                                                                  'reshape_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 582,754\n",
      "Trainable params: 582,754\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue from existing stored run\n",
    "# dir = 'DQN_cp_4f_50ri_3n_pr_cf_tf7_cc_dl_dd_pp2'\n",
    "\n",
    "# replay_mem = joblib.load(f'../{game}/model/{dir}/replay_mem.pkl')\n",
    "\n",
    "# model = tf.keras.models.load_model(f'../{game}/model/{dir}/model/model_latest.keras')\n",
    "# model_tgt = clone_model(model)\n",
    "# model_tgt.set_weights(model.get_weights())\n",
    "\n",
    "# frame_num = 11393496\n",
    "# max_episode_reward = 0\n",
    "# episode_start = 9215\n",
    "# init_done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../script/utils.py:140: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.max(Qs, axis=1)),\n",
      "2024-01-29 23:47:13.949953: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://01d55244-ca61-44c2-a064-e1515e8f8846/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../script/utils.py:140: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.max(Qs, axis=1)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://a6c13597-d1f0-4a90-8ef6-2f9b7daafcb0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../script/utils.py:140: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.max(Qs, axis=1)),\n",
      "../script/utils.py:284: RuntimeWarning: All-NaN axis encountered\n",
      "  ylim = np.nanmax([p.max() for p in pZ]) * 1.05\n",
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/utils.py:284: RuntimeWarning: All-NaN axis encountered\n",
      "  ylim = np.nanmax([p.max() for p in pZ]) * 1.05\n",
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://5a564088-88ef-4149-9644-40ce77f46afe/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3f2f2433-df86-489b-9b31-b9dc310a7b78/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://17cbe5ea-5e41-430f-bb92-8bc53c47fdc9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0f94e7a6-17c2-4da7-a65c-43fafd805eed/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://1c626784-0a8b-4474-bc79-f0a94d678979/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://b3e17e37-5dc2-4110-af4e-833cd058eddd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://d5f49414-5efe-4e3f-b810-e74956b5d79f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://77a0f00d-7e0c-4ead-9aee-854e6bf429f9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://11efec74-dfc8-4c87-844d-4ba083fdcfa5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionjoren/opt/anaconda3/envs/tf-pip-cp-2.12.0/lib/python3.8/site-packages/matplotlib/text.py:1279: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n",
      "../script/replay_memory.py:62: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([self.get_memory(idx, n) for idx in idxs]).T\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 104\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     batch_td_errs, loss \u001b[38;5;241m=\u001b[39m fit_batch_DDQNn_PER(\n\u001b[1;32m    101\u001b[0m         model, model_tgt, action_space, disc_rate, \u001b[38;5;241m*\u001b[39mmini_batch, \n\u001b[1;32m    102\u001b[0m         w_imps, noisy_net, double_learn\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0m \u001b[43mreplay_mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_td_errs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# is there balance between updating ps and having new ps set to max? will updated ps stand a chance?\u001b[39;00m\n\u001b[1;32m    106\u001b[0m episode_train_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/replay_memory.py:188\u001b[0m, in \u001b[0;36mPrioritizedReplayMemory.update_priorities\u001b[0;34m(self, idxs, ps)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_priorities\u001b[39m(\u001b[38;5;28mself\u001b[39m, idxs, ps):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(idxs, ps):\n\u001b[0;32m--> 188\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priority\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/replay_memory.py:181\u001b[0m, in \u001b[0;36mPrioritizedReplayMemory.update_priority\u001b[0;34m(self, buff_idx, p)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_maxtree\u001b[38;5;241m.\u001b[39mupdate(idx, p)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_mintree\u001b[38;5;241m.\u001b[39mupdate(idx, p)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_zero_out_lower_memories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Update the priority given to new memories\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_new_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_new_mem, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_maxtree\u001b[38;5;241m.\u001b[39mget_max())\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/replay_memory.py:169\u001b[0m, in \u001b[0;36mPrioritizedReplayMemory._zero_out_lower_memories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m idxs:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpriorities\u001b[38;5;241m.\u001b[39mupdate(idx, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_maxtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_mintree\u001b[38;5;241m.\u001b[39mupdate(idx, np\u001b[38;5;241m.\u001b[39minf)\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/segment_tree.py:43\u001b[0m, in \u001b[0;36mSegmentTree.update\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m     41\u001b[0m val_child_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx_parent \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     42\u001b[0m val_parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_operator(val_child_l, val_child_r)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_parent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/segment_tree.py:43\u001b[0m, in \u001b[0;36mSegmentTree.update\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m     41\u001b[0m val_child_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx_parent \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     42\u001b[0m val_parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_operator(val_child_l, val_child_r)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_parent\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: SegmentTree.update at line 43 (11 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/segment_tree.py:43\u001b[0m, in \u001b[0;36mSegmentTree.update\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m     41\u001b[0m val_child_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx_parent \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     42\u001b[0m val_parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tree_operator(val_child_l, val_child_r)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_parent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_parent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Projects/Reinforcement learning/atari/script/segment_tree.py:39\u001b[0m, in \u001b[0;36mSegmentTree.update\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx] \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     idx_parent \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     40\u001b[0m     val_child_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx_parent \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m     val_child_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx_parent \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mp_pool = mp.Pool(1)\n",
    "t_lastmax = datetime.now()  # Time since last max train score (controls)\n",
    "i_animation = 0  # Used to toggle between saliency types\n",
    "\n",
    "for episode_num in range(episode_start, total_train_len):\n",
    "    # Start a new game (episode)\n",
    "    init_frame = env.reset()\n",
    "    new_life = True\n",
    "    game_over = False\n",
    "\n",
    "    # Keep track of episode figures\n",
    "    episode_reward = 0\n",
    "    episode_train_cnt = 0\n",
    "    episode_frames = []\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_a_israns = []\n",
    "    episode_Qs = []\n",
    "    episode_pZs = []\n",
    "    episode_losses = []\n",
    "    episode_tderrs = []\n",
    "\n",
    "    # SI specific: skip first 20 frames when new game (40 in total with new-life line)\n",
    "    for _ in range(20):\n",
    "        env.step(0)\n",
    "\n",
    "    # Play episode until game over\n",
    "    while not game_over:\n",
    "        if new_life:\n",
    "            # SI specific: skip first 20 frames when new life\n",
    "            for _ in range(20):\n",
    "                env.step(0)\n",
    "\n",
    "            for _ in range(random.randint(1, 15)):\n",
    "                # Random initialization, to reduce overfitting\n",
    "                frame, _, game_over, info = env.step(sample_ran_action(action_space))\n",
    "            \n",
    "            lives = info['ale.lives']\n",
    "            frame_pp = preprocess_frame_v4(frame, crop_lims)  # Maxpooling not needed for first frame\n",
    "            state = np.stack(state_len * [frame_pp], axis=2)\n",
    "            \n",
    "        # Select action\n",
    "        eps = get_lin_anneal_eps(frame_num - replay_init_sz, eps_init, eps_final, eps_final_frame)\n",
    "        eps = eps if init_done else 1\n",
    "        action, a_isran, Q, pZ = choose_action(model, state, action_space, eps, distr_net, Z, ret_stats=True)\n",
    "        \n",
    "        # Store state and actions variables\n",
    "        episode_frames.append(frame)\n",
    "        episode_states.append(state)\n",
    "        episode_actions.append(action)\n",
    "        episode_a_israns.append(a_isran)\n",
    "        episode_Qs.append(Q)\n",
    "        episode_pZs.append(pZ)\n",
    "        \n",
    "        # Take action and observe transition\n",
    "        prev_frame = frame  # Keep previous frame for max pooling step\n",
    "        frame, reward, game_over, info = env.step(action)\n",
    "        \n",
    "        # Process env response\n",
    "        frame_pp = frame_max_pooling([prev_frame, frame])\n",
    "        frame_pp = preprocess_frame_v4(frame_pp, crop_lims)\n",
    "        state = np.append(state[:, :, 1:], frame_pp[:, :, None], axis=2)\n",
    "        # reward = clip_reward(reward)\n",
    "        new_life = info['ale.lives'] < lives \n",
    "        lives = info['ale.lives']\n",
    "        \n",
    "        # Add new transition to replay memory\n",
    "        transition = (action, reward, game_over or new_life, new_life, frame_pp)  # TODO: inspect game + life\n",
    "        replay_mem.store_memory(transition)\n",
    "                \n",
    "        # Increase transition counters\n",
    "        frame_num += 1\n",
    "        episode_reward += reward\n",
    "        init_done = frame_num >= replay_init_sz  # Is replay initializing done\n",
    "                    \n",
    "        # After init period start replay transitions and train model\n",
    "        if init_done:\n",
    "            \n",
    "            # # Initialize td-errors of init replay mems\n",
    "            # if init_tds and frame_num == replay_init_sz:\n",
    "            #     replay_init_idxs = range(replay_init_sz)\n",
    "            #     replay_init_ps = replay_mem.get_priorities(replay_init_idxs)\n",
    "            #     replay_init_idxs = np.flatnonzero(replay_init_ps)\n",
    "            #     replay_init_mems = replay_mem.get_memories(replay_init_idxs, n_step)[:-1]\n",
    "            #     replay_init_td_errs = td_error(model, model_tgt, action_space, disc_rate, *replay_init_mems)\n",
    "            #     replay_mem.update_priorities(replay_init_idxs, replay_init_td_errs)\n",
    "\n",
    "            # Train model every train_interval\n",
    "            if frame_num % train_interval == 0:\n",
    "                mini_batch = replay_mem.get_sample(batch_sz)\n",
    "                batch_idxs, mini_batch = mini_batch[-1], mini_batch[:-1]\n",
    "                w_imps = replay_mem.get_imps_weights(batch_idxs)\n",
    "                \n",
    "                if distr_net:\n",
    "                    batch_td_errs, loss = fit_batch_DDQNn_PER_DS(\n",
    "                        model, model_tgt, action_space, disc_rate, *mini_batch, w_imps, \n",
    "                        Z, Z_repN, dZ, (V_min, V_max), tgt_zeroing, noisy_net, double_learn\n",
    "                    )\n",
    "                    # print(loss)\n",
    "                else:\n",
    "                    batch_td_errs, loss = fit_batch_DDQNn_PER(\n",
    "                        model, model_tgt, action_space, disc_rate, *mini_batch, \n",
    "                        w_imps, noisy_net, double_learn\n",
    "                    )\n",
    "                replay_mem.update_priorities(batch_idxs, batch_td_errs)\n",
    "                # is there balance between updating ps and having new ps set to max? will updated ps stand a chance?\n",
    "                episode_train_cnt += 1\n",
    "                episode_losses.append(loss)\n",
    "                episode_tderrs.append(np.mean(batch_td_errs))\n",
    "\n",
    "            # Update target model\n",
    "            if frame_num % tgt_update_interval == 0:\n",
    "                model_tgt.set_weights(model.get_weights())\n",
    "\n",
    "    # Log episode statistics\n",
    "    ep_log.append(\n",
    "        episode_num, episode_train_cnt, frame_num, episode_reward, \n",
    "        episode_actions, episode_a_israns, episode_Qs, episode_losses, \n",
    "        episode_tderrs\n",
    "    )\n",
    "\n",
    "    # Output episode animation video every 1000 episodes\n",
    "    if episode_num % 1000 == 0:\n",
    "        # Save model\n",
    "        opath = f'../{game}/model/{model_name}/model/model_{episode_num}.keras'\n",
    "        model.save(opath)\n",
    "\n",
    "        # Save train and test recording\n",
    "        opath = f'../{game}/model/{model_name}/record/record_{episode_num}_train_{episode_reward}.mp4'\n",
    "        sal_type = 'gcam' if i_animation % 2 else 'sal'\n",
    "        mp_pool.apply_async(animate_episode_sal, args=(\n",
    "            model, episode_states, episode_frames, episode_actions, \n",
    "            episode_Qs, action_space, opath, dueling_net, distr_net, (Z, episode_pZs), sal_type)\n",
    "        )\n",
    "        i_animation += 1\n",
    "        # eval_frame, eval_reward = play_episode(model, env, action_space, state_len)\n",
    "        # frames_to_mp4(f'../{game}/model/{model_name}/record/record_{episode_num}_eval_{eval_reward}.mp4', eval_frame)\n",
    "   \n",
    "    # Also output episode video if new max score was attained, and time-delta has been met\n",
    "    elif (episode_reward > max_episode_reward) & (datetime.now() > t_lastmax):\n",
    "        # Store new max reward results\n",
    "        opath = f'../{game}/model/{model_name}/record/record_{episode_num}_train_x_{episode_reward}.mp4'\n",
    "        sal_type = 'gcam' if i_animation % 2 else 'sal'\n",
    "        mp_pool.apply_async(animate_episode_sal, args=(\n",
    "            model, episode_states, episode_frames, episode_actions, \n",
    "            episode_Qs, action_space, opath, dueling_net, distr_net, (Z, episode_pZs), sal_type)\n",
    "        )\n",
    "        i_animation += 1\n",
    "        t_lastmax = datetime.now() + timedelta(minutes=30)\n",
    "        max_episode_reward = episode_reward\n",
    "        \n",
    "    # Backup model and replay memory every 250 episodes\n",
    "    # Note: takes a minute or two, so should not be run frequently \n",
    "    if episode_num % 250 == 0:\n",
    "        # Store replay memory\n",
    "        opath = f'../{game}/model/{model_name}/replay_mem.pkl'\n",
    "        joblib.dump(replay_mem, opath, compress=3)\n",
    "\n",
    "        # Store current model\n",
    "        opath = f'../{game}/model/{model_name}/model/model_latest.keras'\n",
    "        model.save(opath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5 - 3 compr GB for 4508 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
